
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-25T22:29:34.006781+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:29:44.822084+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:29:47.206700+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:30:25.953224+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:30:27.505732+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.5 tokens", "timestamp": "2024-06-25T22:31:34.929998+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:31:36.457482+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-25T22:32:07.280049+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:32:11.080612+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:32:12.672975+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:33:19.815460+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:33:22.101853+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:33:58.263965+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:33:59.886687+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-25T22:34:27.133614+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:34:31.859104+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:34:35.628622+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-25T22:34:50.884283+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-25T22:34:55.254239+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:34:58.407526+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:35:00.104646+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:35:54.435485+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:35:56.214581+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-25T22:43:23.212595+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:43:41.941482+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:43:43.686081+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:45:06.579481+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:45:08.269862+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:48:01.800633+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:48:03.486683+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:48:50.053447+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:48:52.021272+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-25T22:51:34.928536+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:51:36.071446+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "400 Bad Request: The browser (or proxy) sent a request that this server could not understand.", "timestamp": "2024-06-25T22:54:00.814508+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 61, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2152.0 tokens", "timestamp": "2024-06-25T22:55:00.237921+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:55:02.185114+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 25310.25 tokens", "timestamp": "2024-06-25T22:56:53.065992+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"", "timestamp": "2024-06-25T22:56:53.928079+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20966 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}", "timestamp": "2024-06-25T22:56:53.928808+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 25313.25 tokens", "timestamp": "2024-06-25T22:57:46.630921+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"", "timestamp": "2024-06-25T22:57:47.073912+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20968 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}", "timestamp": "2024-06-25T22:57:47.074356+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 25310.25 tokens", "timestamp": "2024-06-25T22:58:20.995466+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"", "timestamp": "2024-06-25T22:58:21.685920+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20966 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}", "timestamp": "2024-06-25T22:58:21.686734+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3805.0 tokens", "timestamp": "2024-06-25T22:58:54.176150+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:58:55.994223+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 23636.0 tokens", "timestamp": "2024-06-25T22:58:59.876157+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"", "timestamp": "2024-06-25T22:59:00.496222+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 21290 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}", "timestamp": "2024-06-25T22:59:00.496698+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3836.0 tokens", "timestamp": "2024-06-25T22:59:20.242448+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:59:22.619272+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 17939.75 tokens", "timestamp": "2024-06-25T22:59:25.494508+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-25T22:59:39.306605+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T15:26:53.831884+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T19:46:12.249166+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
