{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T20:41:43.202542+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T20:42:00.951100+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T20:50:18.875513+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-27T20:51:18.844091+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-27T20:51:20.565154+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T20:51:31.230047+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T21:17:01.957285+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-27T21:31:15.733707+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-27T21:31:18.292813+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T21:31:31.580378+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-27T21:31:36.721960+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-27T23:52:30.896753+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-27T23:52:32.411310+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T00:37:07.420235+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T00:37:09.147676+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T00:39:43.391597+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T00:39:44.792127+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 846.75 tokens", "timestamp": "2024-06-28T00:39:50.930458+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T00:39:54.310766+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1759.5 tokens", "timestamp": "2024-06-28T00:39:57.822161+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T00:39:59.741839+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 9117.25 tokens", "timestamp": "2024-06-28T00:40:04.040772+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T00:40:31.387663+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T00:59:57.093257+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T00:59:58.650870+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T01:06:28.876910+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T01:06:59.220596+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T01:07:26.963449+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T02:17:15.783987+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T02:17:17.204866+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T02:18:48.980985+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T02:18:50.447464+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T02:19:41.050521+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T02:19:42.326626+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 827.25 tokens", "timestamp": "2024-06-28T02:21:05.406314+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T02:21:08.968927+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T02:25:46.018202+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 26, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T02:25:47.463207+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T02:27:31.102052+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T02:27:38.813744+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T15:08:27.637518+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:04:17.994964+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:06:07.447773+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a int.", "timestamp": "2024-06-28T16:06:07.736382+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:06:52.814380+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:07:05.506832+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a int.", "timestamp": "2024-06-28T16:07:07.037490+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:07:57.021264+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a int.", "timestamp": "2024-06-28T16:07:57.799280+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:08:19.928475+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a int.", "timestamp": "2024-06-28T16:08:20.393204+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:08:31.440651+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:13:42.852857+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:13:49.258661+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 62, in getPatentsByIDs\n    results = response.json().get(\"results\")  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T16:13:51.835652+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:14:06.138738+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a int.", "timestamp": "2024-06-28T16:14:06.339746+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:14:16.617695+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a int.", "timestamp": "2024-06-28T16:14:16.813545+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:19:02.350161+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a Response.", "timestamp": "2024-06-28T16:19:02.546324+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:19:13.010024+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 883, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 902, in finalize_request\n    response = self.make_response(rv)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1211, in make_response\n    raise TypeError(\nTypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a Response.", "timestamp": "2024-06-28T16:19:13.857516+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:19:40.033518+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:25:10.276034+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T16:25:10.608084+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:25:23.946625+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:39:38.267376+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T16:39:38.360845+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T16:39:50.890755+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:10:56.526434+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:13:03.283452+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:13:22.153861+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:13:37.914807+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:15:03.625538+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:15:40.859261+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T17:15:40.876454+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:15:42.348099+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "PatentScraper.getSection() missing 1 required positional argument: 'field'", "timestamp": "2024-06-28T17:15:48.115977+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 61, "thread_name": "MainThread"}
{"level": "ERROR", "message": "PatentScraper.getSection() missing 1 required positional argument: 'field'", "timestamp": "2024-06-28T17:15:51.096358+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 61, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:16:53.556088+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:17:00.798530+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:17:00.800432+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:17:03.433546+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:17:12.344223+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:17:18.784776+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:17:21.855916+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:17:21.857107+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:17:23.821896+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:18:08.190454+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:18:09.194178+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:18:09.196170+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:18:11.642191+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:19:05.242465+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:19:08.604679+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:19:11.687909+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 37, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:19:11.689753+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:19:15.438408+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:19:34.592405+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:19:36.664922+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 38, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:19:36.666021+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:19:38.886622+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:19:55.669038+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:19:59.925210+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 37, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:19:59.926903+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:20:02.137691+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:20:14.465504+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:20:32.779283+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 38, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:20:32.781472+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:20:35.209158+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:21:32.157906+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 64, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:21:36.503985+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:21:46.708483+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 64, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:21:46.810667+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:21:51.259710+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:22:39.738680+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:22:40.720214+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:22:47.117128+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:23:05.980363+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:23:06.081042+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:23:12.390378+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:23:12.473052+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:23:16.741403+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:23:23.246344+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:23:23.334186+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:24:26.003163+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 63, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:24:26.103521+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:24:43.635142+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 62, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:24:43.737670+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:24:48.020315+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:25:09.965806+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 971, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 62, in getPatentsByIDs\n    results = response.json()  # decode response\n              ^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/requests/models.py\", line 975, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "timestamp": "2024-06-28T17:25:10.060297+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:25:20.420396+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:26:31.893775+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:26:46.664280+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:27:05.480029+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "'list' object has no attribute 'join'", "timestamp": "2024-06-28T17:27:08.214119+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 61, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:27:44.692646+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "'dict' object has no attribute 'claims'", "timestamp": "2024-06-28T17:27:44.869564+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 36, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1260.5 tokens", "timestamp": "2024-06-28T17:27:44.870666+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T17:27:47.332972+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T17:27:54.286902+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "'list' object has no attribute 'join'", "timestamp": "2024-06-28T17:27:55.341558+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 61, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:07:03.765413+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "'list' object has no attribute 'join'", "timestamp": "2024-06-28T18:07:06.395307+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 61, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:09:51.124986+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1263.25 tokens", "timestamp": "2024-06-28T18:09:52.020688+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:09:54.193374+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:10:01.811000+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-28T18:10:07.596172+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:10:09.170369+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2263.25 tokens", "timestamp": "2024-06-28T18:10:12.309290+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:10:14.591335+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:10:39.633236+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:23:50.872979+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:24:05.691160+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:24:16.103265+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:24:48.067867+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3206.5 tokens", "timestamp": "2024-06-28T18:24:49.277077+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:24:51.531784+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:31:06.547423+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:31:20.059885+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:32:58.947393+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:34:23.771983+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:34:29.102666+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:35:05.176446+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:36:41.989919+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 55, in getPatentsByIDs\n    response = PatentRetrievalFactory.getHandler(data, PatentRetrievalFactory.RequestType.ID)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/factory.py\", line 17, in getHandler\n    raise ValueError(\"not a valid request\")\nValueError: not a valid request\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 59, in getPatentsByIDs\n    return jsonify({\"error\": e}), 400\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/__init__.py\", line 170, in jsonify\n    return current_app.json.response(*args, **kwargs)  # type: ignore[return-value]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 214, in response\n    f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 179, in dumps\n    return json.dumps(obj, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 121, in _default\n    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\nTypeError: Object of type ValueError is not JSON serializable", "timestamp": "2024-06-28T18:37:08.133840+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:37:26.279887+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 55, in getPatentsByIDs\n    response = PatentRetrievalFactory.getHandler(data, PatentRetrievalFactory.RequestType.ID)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/factory.py\", line 17, in getHandler\n    raise ValueError(\"not a valid request\")\nValueError: not a valid request\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 59, in getPatentsByIDs\n    return jsonify({\"error\": e}), 400\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/__init__.py\", line 170, in jsonify\n    return current_app.json.response(*args, **kwargs)  # type: ignore[return-value]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 214, in response\n    f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 179, in dumps\n    return json.dumps(obj, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 121, in _default\n    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\nTypeError: Object of type ValueError is not JSON serializable", "timestamp": "2024-06-28T18:37:28.476609+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:37:36.886376+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 55, in getPatentsByIDs\n    response = PatentRetrievalFactory.getHandler(data, PatentRetrievalFactory.RequestType.ID)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/factory.py\", line 17, in getHandler\n    raise ValueError(\"not a valid request\")\nValueError: not a valid request\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 59, in getPatentsByIDs\n    return jsonify({\"error\": e}), 400\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/__init__.py\", line 170, in jsonify\n    return current_app.json.response(*args, **kwargs)  # type: ignore[return-value]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 214, in response\n    f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 179, in dumps\n    return json.dumps(obj, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 121, in _default\n    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\nTypeError: Object of type ValueError is not JSON serializable", "timestamp": "2024-06-28T18:37:36.892662+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:38:01.698385+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:38:04.032415+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 55, in getPatentsByIDs\n    response = PatentRetrievalFactory.getHandler(data, PatentRetrievalFactory.RequestType.ID)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/factory.py\", line 17, in getHandler\n    raise ValueError(\"not a valid request\")\nValueError: not a valid request\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 59, in getPatentsByIDs\n    return jsonify({\"error\": e}), 400\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/__init__.py\", line 170, in jsonify\n    return current_app.json.response(*args, **kwargs)  # type: ignore[return-value]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 214, in response\n    f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 179, in dumps\n    return json.dumps(obj, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 121, in _default\n    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\nTypeError: Object of type ValueError is not JSON serializable", "timestamp": "2024-06-28T18:38:04.038205+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:38:09.488144+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Exception on /patents/getPatentsByIDs [POST]\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 55, in getPatentsByIDs\n    response = PatentRetrievalFactory.getHandler(data, PatentRetrievalFactory.RequestType.ID)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/factory.py\", line 17, in getHandler\n    raise ValueError(\"not a valid request\")\nValueError: not a valid request\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask_cors/extension.py\", line 176, in wrapped_function\n    return cors_after_request(app.make_response(f(*args, **kwargs)))\n                                                ^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/app/blueprints/patentRetrieval/routes.py\", line 59, in getPatentsByIDs\n    return jsonify({\"error\": e}), 400\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/__init__.py\", line 170, in jsonify\n    return current_app.json.response(*args, **kwargs)  # type: ignore[return-value]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 214, in response\n    f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 179, in dumps\n    return json.dumps(obj, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/alecpalo/Projects/DulanyAI/backend/env/lib/python3.12/site-packages/flask/json/provider.py\", line 121, in _default\n    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\nTypeError: Object of type ValueError is not JSON serializable", "timestamp": "2024-06-28T18:38:09.494316+00:00", "logger": "app", "module": "app", "function": "log_exception", "line": 838, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:38:59.766275+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:39:00.095692+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:39:00.205056+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 148.0 tokens", "timestamp": "2024-06-28T18:39:00.207615+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:39:04.428802+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:39:17.622576+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:39:31.247135+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:39:36.228021+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:39:36.310136+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 148.0 tokens", "timestamp": "2024-06-28T18:39:36.312742+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:39:40.680219+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:40:10.466924+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:40:17.861425+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:41:08.387844+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:41:13.909591+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:41:14.290459+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:41:15.413441+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 35, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 148.0 tokens", "timestamp": "2024-06-28T18:41:15.415700+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:41:20.535017+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:41:22.046623+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:41:25.365519+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 34, "thread_name": "MainThread"}
{"level": "ERROR", "message": "", "timestamp": "2024-06-28T18:41:25.470678+00:00", "logger": "__name__", "module": "scraping", "function": "getSection", "line": 34, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 148.0 tokens", "timestamp": "2024-06-28T18:41:25.477900+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:41:28.214326+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:42:10.394408+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 801.5 tokens", "timestamp": "2024-06-28T18:42:13.632746+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-28T18:42:17.787806+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:42:45.440279+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-28T18:45:53.485738+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:33:19.622274+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 545.75 tokens", "timestamp": "2024-06-29T15:34:49.557303+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:34:50.870399+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:35:10.731141+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3814.25 tokens", "timestamp": "2024-06-29T15:35:11.040922+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:35:13.001182+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:35:52.493024+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2310.25 tokens", "timestamp": "2024-06-29T15:35:52.766795+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:35:55.122260+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:36:41.311314+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2209.0 tokens", "timestamp": "2024-06-29T15:36:43.452376+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:36:45.463818+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1761, 'output_tokens': 115, 'total_tokens': 1876}", "timestamp": "2024-06-29T15:36:45.472053+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 605.25 tokens", "timestamp": "2024-06-29T15:49:44.618675+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:49:46.247042+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 484, 'output_tokens': 106, 'total_tokens': 590}", "timestamp": "2024-06-29T15:49:46.254646+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3703.25 tokens", "timestamp": "2024-06-29T15:49:51.096911+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:49:53.548042+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3450, 'output_tokens': 155, 'total_tokens': 3605}", "timestamp": "2024-06-29T15:49:53.553127+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2495.0 tokens", "timestamp": "2024-06-29T15:49:55.578890+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:49:59.593396+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2450, 'output_tokens': 266, 'total_tokens': 2716}", "timestamp": "2024-06-29T15:49:59.605987+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 5634.25 tokens", "timestamp": "2024-06-29T15:50:03.347548+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:50:08.972715+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 4672, 'output_tokens': 425, 'total_tokens': 5097}", "timestamp": "2024-06-29T15:50:08.978832+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 5631.25 tokens", "timestamp": "2024-06-29T15:50:48.552007+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:50:54.215820+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 4667, 'output_tokens': 501, 'total_tokens': 5168}", "timestamp": "2024-06-29T15:50:54.233059+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:51:55.426784+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 5629.5 tokens", "timestamp": "2024-06-29T15:51:58.961603+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:52:03.495803+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 4665, 'output_tokens': 357, 'total_tokens': 5022}", "timestamp": "2024-06-29T15:52:03.508643+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:52:05.586868+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 5630.75 tokens", "timestamp": "2024-06-29T15:52:24.217342+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T15:52:31.748111+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 4666, 'output_tokens': 568, 'total_tokens': 5234}", "timestamp": "2024-06-29T15:52:31.771959+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:52:42.192271+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 60873.25 tokens", "timestamp": "2024-06-29T15:52:45.768987+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T15:52:46.297189+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.799059 seconds", "timestamp": "2024-06-29T15:52:46.298064+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T15:52:47.388769+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.671951 seconds", "timestamp": "2024-06-29T15:52:47.389180+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T15:52:49.470008+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 60000, Requested 61279. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T15:52:49.472035+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:53:05.690013+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:57:59.281982+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T15:58:10.441346+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 60867.25 tokens", "timestamp": "2024-06-29T15:58:16.962855+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T15:58:17.517003+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.924872 seconds", "timestamp": "2024-06-29T15:58:17.517978+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T15:58:18.744768+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.506605 seconds", "timestamp": "2024-06-29T15:58:18.746162+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T15:58:20.515056+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 61273. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T15:58:20.519940+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T16:09:58.596134+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 605.25 tokens", "timestamp": "2024-06-29T16:10:12.558779+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:10:14.407643+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 479, 'output_tokens': 89, 'total_tokens': 568}", "timestamp": "2024-06-29T16:10:14.422723+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3686.75 tokens", "timestamp": "2024-06-29T16:10:24.730203+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:10:26.816112+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3423, 'output_tokens': 137, 'total_tokens': 3560}", "timestamp": "2024-06-29T16:10:26.823101+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 60865.0 tokens", "timestamp": "2024-06-29T16:10:30.233756+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:10:30.612703+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.937407 seconds", "timestamp": "2024-06-29T16:10:30.615177+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:10:31.815002+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.865739 seconds", "timestamp": "2024-06-29T16:10:31.815827+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:10:34.046253+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 61271. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:10:34.046991+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T16:11:12.034125+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 60865.75 tokens", "timestamp": "2024-06-29T16:11:15.525523+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:11:16.024721+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.945459 seconds", "timestamp": "2024-06-29T16:11:16.025699+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:11:17.242838+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.910175 seconds", "timestamp": "2024-06-29T16:11:17.243205+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:11:19.426687+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 61271. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:11:19.428804+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "Started Up", "timestamp": "2024-06-29T16:12:20.171600+00:00", "logger": "__name__", "module": "run", "function": "<module>", "line": 24, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2200.0 tokens", "timestamp": "2024-06-29T16:12:33.461721+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:12:35.771023+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2088, 'output_tokens': 137, 'total_tokens': 2225}", "timestamp": "2024-06-29T16:12:35.779594+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1123.5 tokens", "timestamp": "2024-06-29T16:12:54.473049+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:13:00.245648+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1211, 'output_tokens': 363, 'total_tokens': 1574}", "timestamp": "2024-06-29T16:13:00.257736+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 36776.75 tokens", "timestamp": "2024-06-29T16:13:03.247723+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:13:03.628222+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.856262 seconds", "timestamp": "2024-06-29T16:13:03.630189+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:13:04.689945+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.541948 seconds", "timestamp": "2024-06-29T16:13:04.690160+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:13:06.478010+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 37068. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:13:06.480383+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3698.0 tokens", "timestamp": "2024-06-29T16:28:25.292307+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:28:27.750051+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3405, 'output_tokens': 137, 'total_tokens': 3542}", "timestamp": "2024-06-29T16:28:27.756003+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 24184.0 tokens", "timestamp": "2024-06-29T16:28:30.599156+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:28:47.204869+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 22168, 'output_tokens': 1217, 'total_tokens': 23385}", "timestamp": "2024-06-29T16:28:47.221054+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 60865.75 tokens", "timestamp": "2024-06-29T16:30:47.509053+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:30:48.043180+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.915442 seconds", "timestamp": "2024-06-29T16:30:48.044569+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:30:49.215162+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.557475 seconds", "timestamp": "2024-06-29T16:30:49.215438+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:30:51.012977+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 61271. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:30:51.013412+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 36770.75 tokens", "timestamp": "2024-06-29T16:31:16.863734+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:31:17.217105+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.895710 seconds", "timestamp": "2024-06-29T16:31:17.218398+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:31:18.371657+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.573409 seconds", "timestamp": "2024-06-29T16:31:18.373526+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:31:20.161691+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 37062. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:31:20.163724+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2931.0 tokens", "timestamp": "2024-06-29T16:31:50.917878+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:31:53.171464+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3032, 'output_tokens': 137, 'total_tokens': 3169}", "timestamp": "2024-06-29T16:31:53.176010+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3509.0 tokens", "timestamp": "2024-06-29T16:31:53.456378+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:31:56.148202+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3400, 'output_tokens': 137, 'total_tokens': 3537}", "timestamp": "2024-06-29T16:31:56.153124+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 37628.0 tokens", "timestamp": "2024-06-29T16:31:56.675420+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:31:57.058330+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.779294 seconds", "timestamp": "2024-06-29T16:31:57.058714+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:31:58.199423+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.796139 seconds", "timestamp": "2024-06-29T16:31:58.200362+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:32:00.302090+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 37806. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:32:00.302478+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 41276.5 tokens", "timestamp": "2024-06-29T16:32:00.840462+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:32:01.319570+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.978927 seconds", "timestamp": "2024-06-29T16:32:01.319976+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:32:02.514817+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.516810 seconds", "timestamp": "2024-06-29T16:32:02.515303+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:32:04.248083+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 41401. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:32:04.248431+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3717.5 tokens", "timestamp": "2024-06-29T16:32:38.223767+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:32:42.019615+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3616, 'output_tokens': 137, 'total_tokens': 3753}", "timestamp": "2024-06-29T16:32:42.023976+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1592.75 tokens", "timestamp": "2024-06-29T16:32:42.247689+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:32:44.574161+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1377, 'output_tokens': 137, 'total_tokens': 1514}", "timestamp": "2024-06-29T16:32:44.578452+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1972.75 tokens", "timestamp": "2024-06-29T16:32:44.764027+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:32:47.241692+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1771, 'output_tokens': 137, 'total_tokens': 1908}", "timestamp": "2024-06-29T16:32:47.247450+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1874.25 tokens", "timestamp": "2024-06-29T16:32:47.519147+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:32:49.799884+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1630, 'output_tokens': 137, 'total_tokens': 1767}", "timestamp": "2024-06-29T16:32:49.804783+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2459.0 tokens", "timestamp": "2024-06-29T16:32:50.043020+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:32:52.128509+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2567, 'output_tokens': 137, 'total_tokens': 2704}", "timestamp": "2024-06-29T16:32:52.131864+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 23438.5 tokens", "timestamp": "2024-06-29T16:32:52.612146+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:33:19.507030+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 24818, 'output_tokens': 1438, 'total_tokens': 26256}", "timestamp": "2024-06-29T16:33:19.530917+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 44909.75 tokens", "timestamp": "2024-06-29T16:33:41.466870+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:33:41.921933+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.803996 seconds", "timestamp": "2024-06-29T16:33:41.922359+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:33:42.960051+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.549274 seconds", "timestamp": "2024-06-29T16:33:42.960747+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:33:44.761631+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 45128. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:33:44.762501+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 46276.25 tokens", "timestamp": "2024-06-29T16:34:00.658524+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:34:01.376933+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.923866 seconds", "timestamp": "2024-06-29T16:34:01.378306+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:34:02.598726+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.940680 seconds", "timestamp": "2024-06-29T16:34:02.599919+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:34:04.855665+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 46519. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:34:04.856042+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 17321.5 tokens", "timestamp": "2024-06-29T16:34:23.281459+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:34:39.571466+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 16513, 'output_tokens': 1238, 'total_tokens': 17751}", "timestamp": "2024-06-29T16:34:39.605346+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 43063.25 tokens", "timestamp": "2024-06-29T16:34:44.716214+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:34:45.405856+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 0.935355 seconds", "timestamp": "2024-06-29T16:34:45.406216+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:34:46.544927+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "Retrying request to /chat/completions in 1.907913 seconds", "timestamp": "2024-06-29T16:34:46.545575+00:00", "logger": "openai._base_client", "module": "_base_client", "function": "_retry_request", "line": 1047, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "timestamp": "2024-06-29T16:34:48.718115+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-auoaVTjD1KeMmDkPBKRg7Dyi on tokens per min (TPM): Limit 30000, Requested 43701. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}", "timestamp": "2024-06-29T16:34:48.718854+00:00", "logger": "__name__", "module": "routes", "function": "getCitation", "line": 83, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 24185.5 tokens", "timestamp": "2024-06-29T16:35:20.077088+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T16:35:36.072917+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 22168, 'output_tokens': 1127, 'total_tokens': 23295}", "timestamp": "2024-06-29T16:35:36.091520+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 605.25 tokens", "timestamp": "2024-06-29T20:48:09.130724+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:11.867270+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 479, 'output_tokens': 102, 'total_tokens': 581}", "timestamp": "2024-06-29T20:48:11.874102+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2939.5 tokens", "timestamp": "2024-06-29T20:48:22.571279+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:24.951726+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3045, 'output_tokens': 150, 'total_tokens': 3195}", "timestamp": "2024-06-29T20:48:24.962145+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3517.5 tokens", "timestamp": "2024-06-29T20:48:25.209460+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:27.634885+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3413, 'output_tokens': 150, 'total_tokens': 3563}", "timestamp": "2024-06-29T20:48:27.641139+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3695.25 tokens", "timestamp": "2024-06-29T20:48:28.293264+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:30.808437+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 3436, 'output_tokens': 150, 'total_tokens': 3586}", "timestamp": "2024-06-29T20:48:30.813540+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2208.5 tokens", "timestamp": "2024-06-29T20:48:30.982910+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:33.085007+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2101, 'output_tokens': 150, 'total_tokens': 2251}", "timestamp": "2024-06-29T20:48:33.091469+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2812.5 tokens", "timestamp": "2024-06-29T20:48:33.405169+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:35.280060+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2738, 'output_tokens': 141, 'total_tokens': 2879}", "timestamp": "2024-06-29T20:48:35.290416+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1601.25 tokens", "timestamp": "2024-06-29T20:48:35.461287+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:47.908844+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "ERROR", "message": "Failed to parse JSON from completion: Invalid json output: ```json\n{\n\"data\": {\n    \"Encodes omega-3 fatty acid desaturase enzyme.\": 0.5,\n    \"Enhances lipid retention in Nannochloropsis.\": 0.1,\n    \"Enhances lipid retention in Chlorella.\": 0.1,\n    \"Related to W-3 FAD gene.\": 0.2,\n    \"Filed information on W-3 FAD gene.\": 0.3,\n    \"Studied in micro algae.\": 0.2,\n    \"Associated with omega-3 fatty acids.\": 0.6,\n    \"Improves lipid profiles in algae.\": 0.1\n},\n}\n```", "timestamp": "2024-06-29T20:48:47.912916+00:00", "logger": "__name__", "module": "routes", "function": "extractSpecificPatentMetrics", "line": 57, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 3106.75 tokens", "timestamp": "2024-06-29T20:48:48.125268+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:50.062593+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2941, 'output_tokens': 150, 'total_tokens': 3091}", "timestamp": "2024-06-29T20:48:50.065070+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2467.5 tokens", "timestamp": "2024-06-29T20:48:50.208049+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:52.824901+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2580, 'output_tokens': 150, 'total_tokens': 2730}", "timestamp": "2024-06-29T20:48:52.833243+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2571.75 tokens", "timestamp": "2024-06-29T20:48:52.978174+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:55.179961+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2219, 'output_tokens': 150, 'total_tokens': 2369}", "timestamp": "2024-06-29T20:48:55.186245+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1981.25 tokens", "timestamp": "2024-06-29T20:48:55.360590+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:48:57.023411+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1784, 'output_tokens': 150, 'total_tokens': 1934}", "timestamp": "2024-06-29T20:48:57.029145+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1601.25 tokens", "timestamp": "2024-06-29T20:51:23.094116+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:51:26.013003+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1390, 'output_tokens': 150, 'total_tokens': 1540}", "timestamp": "2024-06-29T20:51:26.015151+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1813.0 tokens", "timestamp": "2024-06-29T20:57:18.426231+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:57:26.396786+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2111, 'output_tokens': 530, 'total_tokens': 2641}", "timestamp": "2024-06-29T20:57:26.415760+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2488.0 tokens", "timestamp": "2024-06-29T20:57:26.796881+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:57:33.298350+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2570, 'output_tokens': 480, 'total_tokens': 3050}", "timestamp": "2024-06-29T20:57:33.314416+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2603.5 tokens", "timestamp": "2024-06-29T20:57:33.806297+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:57:40.685436+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2553, 'output_tokens': 517, 'total_tokens': 3070}", "timestamp": "2024-06-29T20:57:40.702133+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1123.5 tokens", "timestamp": "2024-06-29T20:57:41.461481+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:57:46.800505+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1211, 'output_tokens': 404, 'total_tokens': 1615}", "timestamp": "2024-06-29T20:57:46.808698+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1765.0 tokens", "timestamp": "2024-06-29T20:57:47.069343+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:57:52.014832+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1865, 'output_tokens': 442, 'total_tokens': 2307}", "timestamp": "2024-06-29T20:57:52.021826+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 519.5 tokens", "timestamp": "2024-06-29T20:57:52.250843+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:57:57.374586+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 509, 'output_tokens': 394, 'total_tokens': 903}", "timestamp": "2024-06-29T20:57:57.389465+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 2093.25 tokens", "timestamp": "2024-06-29T20:57:57.686790+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:58:03.841283+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 2110, 'output_tokens': 454, 'total_tokens': 2564}", "timestamp": "2024-06-29T20:58:03.857353+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1403.0 tokens", "timestamp": "2024-06-29T20:58:04.140998+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:58:08.199798+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1715, 'output_tokens': 410, 'total_tokens': 2125}", "timestamp": "2024-06-29T20:58:08.213946+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 1480.75 tokens", "timestamp": "2024-06-29T20:58:08.508701+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:58:14.694177+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 1333, 'output_tokens': 559, 'total_tokens': 1892}", "timestamp": "2024-06-29T20:58:14.707440+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 899.25 tokens", "timestamp": "2024-06-29T20:58:14.965510+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T20:58:18.177772+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 903, 'output_tokens': 293, 'total_tokens': 1196}", "timestamp": "2024-06-29T20:58:18.190119+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
{"level": "INFO", "message": "This request used about 605.25 tokens", "timestamp": "2024-06-29T21:06:11.767220+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 27, "thread_name": "MainThread"}
{"level": "INFO", "message": "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"", "timestamp": "2024-06-29T21:06:13.823599+00:00", "logger": "httpx", "module": "_client", "function": "_send_single_request", "line": 1026, "thread_name": "MainThread"}
{"level": "INFO", "message": "{'input_tokens': 479, 'output_tokens': 110, 'total_tokens': 589}", "timestamp": "2024-06-29T21:06:13.828273+00:00", "logger": "__name__", "module": "llmRequests", "function": "makeRequest", "line": 55, "thread_name": "MainThread"}
Started Up
